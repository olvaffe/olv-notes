Kernel Boot on x86-64
=====================

## `boot/compressed/`

- `vmlinux` in the root directory is objcopy'ed to `boot/compressed/` as
  `$(obj)/vmlinux.bin`, and is stripped
- `$(obj)/vmlinux.bin` is gzipped as `$(obj)/vmlinux.bin.gz`.
- generate `piggy.S` that `.incbin $(obj)/vmlinux.bin.gz` with the help of
  `mkpiggy`
- `piggy.S` and other sources under the directory are compiled into
  `$(obj)/vmlinux` (not the one in the root directory), using
  `$(obj)/vmlinux.lds`.

## `boot/`

- `$(obj)/compressed/vmlinux` is objcopy'ed here as `$(obj)/vmlinux.bin`.
- `setup.elf` is compiled from sources under this directory, using `setup.ld`.
- `setup.elf` is stripped as `setup.bin`
- `setup.bin` and `vmlinux.bin` are put together by `tools/build` to create
  `bzImage`.
  - `build` patches the header to give correct values.

## `CONFIG_EFI_STUB`

- `efi_pe_entry` of `x86-stub.c` is the entrypoint
  - `LOADED_IMAGE_PROTOCOL` is used to get `efi_loaded_image_t` from the
    bootloader
  - `boot_params` is allcoated and initialized from `efi_loaded_image_t`
  - `efi_stub_entry` jumps to the kernel image
    - `efi_decompress_kernel` self-decompresses
    - `efi_load_initrd` loads initramfs
    - `enter_kernel` jumps to the entrypoint
      - `%rsi` holds `boot_params`
- `SYM_CODE_START_NOALIGN(startup_64)` is the entrypoint
  - `%rsi` holds `boot_params`
  - `__startup_64`
  - it jumps to `common_startup_64` at the end
- `SYM_INNER_LABEL(common_startup_64, SYM_L_LOCAL)`
  - `early_setup_idt`
  - it calls `x86_64_start_kernel` at the end
- `x86_64_start_kernel`
  - `copy_bootdata` copies `boot_params`
    - it also copies cmdline to `boot_command_line`
  - it calls `start_kernel` at the end
    - `setup_arch` calls `reserve_initrd` to init `initrd_start` and
      `initrd_end`

## Bootloader

- `Document/x86/boot.rst`
- Bootloader loads first some KB of the kernel to a place it chooses.
  - It contains the header and real-mode code.
- Bootloader also loads protect-mode code to the right place.
  - The right place is given by `code32_start` of the header
  - It is usually in the high memory, which is not accessible in real mode
  - So the bootloader might either enter protected mode
  - Or, use `Int 15/AH=87h`, <http://www.ctyme.com/intr/rb-1527.htm>
- It then jumps to the start of the header.

## 16-bit Real-Mode

- It starts from `boot/header.S`.  The first two bytes of the header is a jump
  instruction.  It jumps to `start_of_setup`, to call `main` in `boot/main.c`.
- `protected_mode_jump` called by `go_to_protected_mode` enters the 32-bit
  protected mode and jumps to the start of protected-mode code at
  `code32_start`

## 32-bit Protected Mode and 64-bit Long Mode

- It usually starts from `startup_32` in `boot/compressed/head_64.S`
  - it sets up identity page tables, enables paging, enters long mode
    (64-bit mode), and jumps to `startup_64`
- `startup_64` decompresses the kernel
  - The compressed kernel is loaded at `code32_start` by current bootloader,
    which is hardcoded at `0x100000`.  Future bootloader should load it to
    `pref_address`, which is `LOAD_PHYSICAL_ADDR`.
  - The decompressor wants to decompress it to `LOAD_PHYSICAL_ADDR` for
    non-relocatable kernel or decompress in-place for relocatable kernel.
  - The decompressed kernel is an elf image.  It is parsed and its elf programs
    are copied in-place to the right place.
  - It has in `ebp` the loaded address (`code32_start` or `pref_address`) and in
    `ebx` the `LOAD_PHYSICAL_ADDR` (usually the same, 0x1000000 since 2.6.31).
    The size difference between the uncompressed/compressed images plus
    necessary decompress offset is in `z_extract_offset`.
  - It copies the compressed kernel from loaded address to physical address
    (`ebx`).  It is copied backward to allow in-place copy.  It is copied to the
    end of physical address to allow in-place decompressions.
- finally, it jumps to another `startup_64` in `kernel/head_64.S`
  - with identity-mapped address space
  - the stack is set up by `movq initial_stack(%rip), %esp`
  - jumps to `x86_64_start_kernel`, which calls `start_kernel`.

## Timers

- TSC and LAPIC timers
  - TSC frequency is cpu clock frequency
  - `RDTSC` has the current counter value
  - LAPIC timers are per-core timers that fire when TSC exceeds the programmed
    values
    - this is the TSC-deadline mode
    - LAPIC timers support other modes
- HPET
  - designed to replace PIT
- ACPI timer
  - do not use
- PIT timer
  - do not use
  - may be emulated by HPET
- RTC timer
  - do not use

## Kernel

- Linux x86 boot protocol is defined in `Documentation/x86/boot.rst`
  - the bootloader should load the kernel to certain memory addresses
  - the first sector of the kernel is used for communications between the
    bootloader and the kernel: how big is the kernel?  where did the bootloader
    write the commandline to?  where was the initramfs loaded?
- After arch specific early boot code, `start_kernel` in `init/main.c` is called.
  - it prints a "Linux version" banner
  - it calls `setup_arch`
  - it spawns a thread to run `kernel_init`
    - the thread is the first and has pid 1
    - it will later `do_execve("/sbin/init")`
- see <../init.md>

## Resources

- `/proc/iomem`
- During `setup_arch`, `e820__reserve_resources` is called to turn
  non-reserved e820 entries into resources.
  - Incidentally, `e820__setup_pci_gap` decides `pci_mem_start`, which finds a gap
    between e820 entries that is high and large enough.
- In subsys initcalls, `pcibios_init` is called.  It calls
  `pcibios_resource_survey` to allocate PCI resources.  That calls
  `e820__reserve_resources_late` to insert the rest of e820 entries.

## IDT Initialization

- in real mode, the cpu uses Interrupt Vector Table (IVT)
- in protected mode or long mode, the cpu uses Interrupt Descriptor Table
  (IDT)
- IDT entry size is 256 with the first 32 entries reserved for processor
  exceptions (traps)
  - `IDT_ENTRIES` and `NR_VECTORS` is 256
  - `NUM_EXCEPTION_VECTORS` and `FIRST_EXTERNAL_VECTOR` is 32
  - traps include
    - `X86_TRAP_DE` divide-by-zero
    - `X86_TRAP_NMI` NMI
    - `X86_TRAP_OF` overflow
    - `X86_TRAP_PF` page fault
    - and others
- `NR_IRQS` is `NR_VECTORS + IO_APIC_VECTOR_LIMIT`
  - `FIRST_SYSTEM_VECTOR` is `LOCAL_TIMER_VECTOR` which is 0xec
- In `go_to_protected_mode`,
  - `realmode_switch_hook` issues `cli` to disable local irq
  - `mask_all_interrupts` masks out all irq in PIC
  - `setup_idt` loads a null IDT
  - jumps to protect mode and call `startup_32`
- In `startup_32` in `boot/compressed/head_64.S`,
  - it jumps to long mode (64-bit) and call `startup_64`
- In `startup_64` in `boot/compressed/head_64.S`,
  - `load_stage1_idt` loads `boot_idt` which is still all zeros
  - `load_stage2_idt` initializes `X86_TRAP_PF` to `boot_page_fault` which
    calls `do_boot_page_fault`
  - it extracts the kernel and jumps to another `startup_64`
- In `startup_64` in `kernel/head_64.S`,
  - `startup_64_setup_env` loads `bringup_idt_table` which is all zeros
  - `early_setup_idt` loads `bringup_idt_table` again after switching page
    table
  - it calls `x86_64_start_kernel`
- In `x86_64_start_kernel`,
  - `idt_setup_early_handler` loads `idt_table`
    - `idt_table` is initialized from `early_idt_handler_array`, where all
      traps are handled by `early_idt_handler_common` that sets up early
      pgtable and works around bugs
  - at the end, `start_kernel` is called
- In `setup_arch` called by `start_kernel`,
  - `idt_setup_early_traps` updates some trap handlers in `idt_table`
  - `idt_setup_early_pf` updated `X86_TRAP_PF` handler
- In `trap_init` called by `start_kernel`,
  - `idt_setup_traps` sets the final trap handlers
- In `start_kernel` after `trap_init`,
  - `early_irq_init` initializes the kernel IRQ subsystem and calls
    `arch_early_irq_init`
  - `init_IRQ` initializes all HW IRQs after traps.
    - `x86_init.irqs.intr_init` points to `native_init_IRQ`
    - `idt_setup_apic_and_irq_gates` initializes that part of IDT from
      `irq_entries_start` whose handlers are all `asm_common_interrupt`
      - `DECLARE_IDTENTRY_IRQ(X86_TRAP_OTHER, common_interrupt)` expands to
        `idtentry_irq` which defines `asm_common_interrupt`
      - it calls `common_interrupt` which calls `generic_handle_irq_desc`
  - `local_irq_enable` is called

## Context Switches

- current task
  - `current` is defined to `get_current`
  - it returns `current_task` per-cpu variable
- task switch
  - `switch_to` is defined to `__switch_to_asm`
  - saves `prev` registers to `prev` stack
  - switches to `next` stack
  - restores `next` registers from `next` stack
  - `__switch_to`
    - sets `current_task` to `next_p`
    - sets `cpu_current_top_of_stack` to `task_top_of_stack(next_p)`
    - others
  - because the stack has been switched, when we return from `__switch_to`, we
    return to the last frame of `next`
- new task
  - other than `init_task`, all tasks are created by `copy_process`
  - `copy_process` calls the arch-specific `copy_thread`
    - `copy_thread` sets `ret_addr` to `ret_from_fork`
    - after `__switch_to` switches to the new task, it will return to
      `ret_from_fork`
  - `ret_from_fork`
    - calls `schedule_tail` to set up the fresh new task
    - if `rbx` is set, this is a kernel thread and `rbx` is the starting
      function
      - this is set up in `copy_thread`
    - otherwise, this is a userspace task
      - `syscall_exit_to_user_mode` is generic code
      - `swapgs_restore_regs_and_return_to_usermode`
        - restores entry trampoline regs
        - switches to entry trampoline stack
        - restores IRET frame
        - `iretq`
- entry trampoline stack
  - the entry trampoline stack is the stack the kernel uses after entry from
    userspace or before exit to userspace
  - `cpu_init` calls `load_sp0` to set `sp0` to `cpu_entry_stack(cpu)`
    - it never changes after initialization
  - `cpu_init_exception_handling` sets TSS
    - some exceptions use `cea_exception_stacks` stacks
    - others just use `sp0`, that is, `cpu_entry_stack`
- context switch
  - `idtentry` is the macro that defines the entrypoint (`asm_foo`) for
    exception vectors
    - `error_entry` saves register to the current stack
      - if entering from kernel, this is kernel stack of the current task
      - if entering from userspace, this is trampoline stack of the current
        cpu
        - `sync_regs` copies the contents from the trampoline stack to the
          kernel stack of the current task and returns back to `asm_foo`

## External Interrupts

- when an external interrupt is received, CPU disables interrupts (`cli`) and
  pushes some flags registers to the stack automatically
- IDT entries from `FIRST_EXTERNAL_VECTOR` to `NR_VECTORS` are initialized
  from `irq_entries_start`
  - `DECLARE_IDTENTRY_IRQ(X86_TRAP_OTHER, common_interrupt)`
    - this expands to `idtentry_irq` which expands to `idtentry` which expands
      to `asm_common_interrupt` when expanded by the assembler
    - `error_entry` saves registers onto the kernel stack
  - `DEFINE_IDTENTRY_IRQ(common_interrupt)`
    - this defines `common_interrupt` and `__common_interrupt`
  - `asm_common_interrupt`, the handler, is defined by `idtentry_irq`
    - `error_entry`
      - `PUSH_AND_CLEAR_REGS` pushes registers to the current stack
        - it could be the entry trampoline stack or the kernel stack of
          `current`
      - `sync_regs` copies the saved registers to the kernel stack of `current`
      - `sync_regs` returns the kernel stack
    - switch the stack to the return value of `error_entry` which is always
      the kernel stack of `current`
    - call `common_interrupt`
  - `common_interrupt`
    - `irqentry_enter` is the generic entry enter code
    - `run_irq_on_irqstack_cond` calls `__common_interrupt` on either the
      kernel stack or on irqstack `pcpu_hot.hardirq_stack_ptr`
    - `irqentry_exit` is the generic entry exit code
  - `__common_interrupt`
    - look up `vector_irq[vector]` to get `struct irq_desc`
    - call `generic_handle_irq_desc` on the irq desc
- IRQ handling
  - `early_irq_init` sets up irq to `irq_desc` mappings
  - `idt_setup_apic_and_irq_gates` sets up IDT
  - `irq_set_chip_and_handler` updates an `irq_desc` to point to the specified
    `irq_chip` and handler
  - when a hw irq happens, `IDT -> common_interrupt -> irq_desc::handle_irq`
  - the handler acks the irq, and calls each action in the `irq_desc::action`
    list
  - `request_irq` registers an action to an `irq_desc`

## Page Fault

- in `idt_setup_early_pf`, `asm_exc_page_fault` is used for `X86_TRAP_PF`
- `DECLARE_IDTENTRY_RAW_ERRORCODE(X86_TRAP_PF, exc_page_fault)` expands to
  `idtentry` to define `asm_exc_page_fault`
  - it is very standard as other exception vectors
  - `call error_entry`
  - `call exc_page_fault`
  - `jmp error_return`
- `DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)` defines `exc_page_fault`
  - `irqentry_enter`
  - `handle_page_fault` calls `do_user_addr_fault`
  - `irqentry_exit`
- `do_user_addr_fault`
  - `mm` is from `current->mm`
  - `mmap_read_trylock` locks the mm for read access
  - `find_vma` returns the vma for the fault address
    - `expand_stack` takes care of automatic stack grow
  - `handle_mm_fault` is the generic fault function

## Interrupt Descriptor Table

- `asm/segment.h`
  - Interrupt Descriptor Table, IDT
    - there are 256 (`IDT_ENTRIES`) descriptors
    - the table is loaded with `lidt` instruction
    - each descriptor is 16 bytes
    - it describes when an interrupt happens, where to jump to (and the
      priviledge level, irq disable, whether to save context, etc.)
    - the jump targets are defined by `idtentry` in `entry_64.S`
- `asm/irq_vectors.h`
  - vectors
    - there are 256 (`NR_VECTORS`) vectors
    - vector 0..31 are for system traps and exceptions and are hardcoded by CPU
    - vector 32..127 are for device interrupts
      - `FIRST_EXTERNAL_VECTOR` is 32
      - `ISA_IRQ_VECTOR` uses 48..63 for ISA
      - these are initialized from `irq_entries_start` and call `do_IRQ` after
      	some setups
    - vector 128 is for legacy int80 syscall
      - `IA32_SYSCALL_VECTOR` is 128
    - vector 129..230ish is unused
    - the rest is for kernel-defined IPIs
      - starting from `FIRST_SYSTEM_VECTOR`

## Syscalls

- x86-64 has an instruction, `syscall`, that loads rip from `MSR_LSTAR` MSR.
  - the intr also loads cs/ss from `MSR_STAR` MSR
  - `syscall_init` initializes `MSR_LSTAR` to `entry_SYSCALL_64` and
    `MSR_STAR` to `__KERNEL_CS`
- `entry_SYSCALL_64` does
  - save user rsp and set it to `cpu_current_top_of_stack`
  - construct `pt_regs` on stack
  - call `do_syscall_64`
    - enable IRQ for kernel mode
    - execute the syscall
    - disable IRQ for user mode
  - `iretq` or `sysretq` that restore flags

## Privilege

- In an IDT entry, there is a 16-bits segment selector and a 2-bits DPL.
  - The selector decides thew new segment
  - The DPL decides the lowest privilege that can interrupt
  - `cs` gives CPL.  If `CPL >= PL of new segment` and `CPL <= DPL`, interrupt
    succeeds.  The idea is so that the interrupt handler has higher privilege
    than the program caused the interrupt and the program has enough privilege
    to cause the interrupt.
  - `cs:eip` becomes `segment selector:offset` in the IDT entry.
- In `setup_gdt` in `boot/pm.c`, `GDT_ENTRY_BOOT_CS` and `GDT_ENTRY_BOOT_DS`
  GDT have acess flags `0x9b` and `0x93`.  Both indicate highest privilege.

## x86-64 Paging Table

- 4-level paging
  - 48-bit virtual address and 46-bit physical address
  - CR3 bit 51..12: PA to the 4KB-aligned PML4 (Page Map Level 4) table
    - 512 64-bit entries called PML4Es
  - VA bit 47..39: select one of the 512 PML4Es
    - each PML4E contains a PA to a PDP (page-directory-pointer) table
    - there are 512 64-bit entries called PDPEs
  - VA bit 38..30: select one of the 512 PDPEs
    - each PDPE contains a PA to a PD (page directory)
    - there are 512 64-bit entries called PDEs
  - VA bit 29..21: select one of the 512 PDEs
    - each PDE contains a PA to a page table
    - there are 512 64-bit entries called PTEs
  - VA bit 20..12: select one of the 512 PTEs
    - each PTE contains a PA to a page
  - VA bit 11..0: 12-bit offset into the page
- 5-level paging
  - 57-bit virtual address and 52-bit physical address
  - CR3 bit 51..12: PA to the 4KB-aligned PML5 (Page Map Level 5) table
  - VA bit 56..48: select one of the 512 PML5Es
    - each PML5E contains a PA to a PML4 table
    - there are 512 64-bit entries called PML5Es
- All entries at all levels have a similar (but different) format
  - bit 63..52: 12-bit for flags
    - bit 63 is NX (no execute)
    - more
  - bit 51..12: 40-bit pfn
  - bit 11..0: 12-bit for flags
    - bit 4 is page cache disabled (for the next table; or page when it is PTE)
    - bit 3 is page write through (for the next table; or page when it is PTE)
    - bit 1 is writable
    - bit 0 is present
    - more

## Nested Paging

- AMD-V Nested Paging White Paper
  - <http://developer.amd.com/wordpress/media/2012/10/NPT-WP-1%201-final-TM.pdf>
- SW shadow page table
  - guest has a guest page table
  - hypervisor has a shadow page table
  - the HW MMU uses the shadow page table when the guest is active
  - the guest page table is marked read-only
    - whenever the guest updates it, it traps into the hypersor
    - the hypervisor updates both the guest and the shadow page tables
- HW nested page table
  - HW MMU uses the guest page table directly
  - an additional nested page table set up by the hypervisor is used to
    translate guest physical address to host physical address
  - for a 4-level paging walk in guest results in 5 walks in the nested page
    walker (to access PML4, PDPE, PDE, PTE, and the page)
- memory type selection
  - MTRR
    - 0x0: UC
    - 0x1: WC
    - 0x4: WT
    - 0x5: WP (write protected)
    - 0x6: WB
    - Linux does not use MTRR.  BIOS(?) sets MTRRdefType to 0xc06, which means
      WB by default.
  - `IA32_PAT` MSR has 8 3-bit page attribute fields, PA0..PA7
    - each field can have one of these values
      - 0x0: UC
      - 0x1: WC
      - 0x4: WT
      - 0x5: WP
      - 0x6: WB
      - 0x7: UC-
    - In `pat_init`, they are initialized to
      - PA0: WB
      - PA1: WC
      - PA2: UC-
      - PA3: UC
      - PA4: WB (unused)
      - PA5: WP
      - PA6: UC- (unused)
      - PA7: WT
  - PTE's bit 7 (PAT), 4 (PCD), and 3 (PWT) form a 3-bit index that is used to
    select from PA0..PA7
    - `pgprot_writecombine` maps to PA1
  - In EPT (extended page table, used by KVM), MTRR is ignored and EPT bits
    5:3 replace MTRR (0: UC, 1: WC, 4: WT, 5 WP, 6: WB)
    - `vmx_get_mt_mask`

## x86-64: overview

- get physical memory map from BIOS
  - `e820__memory_setup` calls `e820__memory_setup_default`
  - the table is copied from `boot_params` to `e820_table`
    - `boot_params` was set up with `detect_memory_e820`
- update e820 table to reserve setup data
  - `e820__reserve_setup_data` finds setup data from `boot_params` and updates
    `e820_table`
- prepare page table storage
  - the storage uses the brk data section of the kernel binary
  - `early_alloc_pgt_buf` claims a area from the brk section
  - `alloc_low_pages` will use the area first before using memblock
- setup memblock allocator according to e820
  - `e820__memblock_setup`
  - `memblock_add` adds a usable RAM range to memblock
  - `memblock_reserve` marks a usable RAM range reserved
  - regions containing BIOS or EFI data should be marked reserved
  - an allocation from memblock finds a usable range and mark it reserved
- setup linear map
  - `init_mem_mapping`
  - `probe_page_size_mask` checks if GB huge page is possible (yes)
  - `init_mm` is the mm, and `init_mm.pgd` is `init_top_pgt`
    - i.e., the 4KB space for pgt is in the data section
  - the storage of the reset of the page tables is allocated using
    `alloc_low_page`, which uses the area prepared by `early_alloc_pgt_buf`
- set up `struct page` array and set up zones for buddy allocator
  - `x86_init.paging.pagetable_init` points to `native_pagetable_init`
  - `sparse_init_nid` allocates the `struct page` array
    - `sparse_buffer_init` allocates the storage from memblock
  - `zone_sizes_init`
- free pages from memblock to buddy allocator
  - `memblock_free_all` called by `mem_init`
- on a machine with 32G of memory,
  - e820 reports about 31.7G usable
  - `zone_sizes_init` reports a similar amount of memory
  - `mem_init_print_info` reports a similar amount of memory
    - `get_num_physpages()` is 31.7G but `totalram_pages()` is 30.8G
    - the delta is reported as reserved and consists of
      - kernel text, data, bss, and init sections
      - `struct page` array
      - hw carved-out regions
    - free pages are limited to those in lowmem thanks to
      `CONFIG_DEFERRED_STRUCT_PAGE_INIT`
  - `free_reserved_area` frees up some reserved memory
    - init section, initrd, etc.
    - this increases `totalram_pages()`
  - `grep MemTotal /proc/meminfo` reports 31.06G
    - it reports `totalram_pages()`

## x86-64

- Kconfig
  - `64BIT`
  - `X86_64`
  - `MTRR`
  - `X86_PAT`
  - `DYNAMIC_MEMORY_LAYOUT`
  - `SPARSEMEM_VMEMMAP`
  - `SPARSEMEM_EXTREME`
  - `SPARSEMEM`
  - `PHYS_ADDR_T_64BIT`
  - `NEED_SG_DMA_LENGTH`
  - `NEED_DMA_MAP_STATE`
  - `ARCH_DMA_ADDR_T_64BIT`
  - `SLUB`
  - `HAVE_MEMBLOCK_NODE_MAP`
  - `HAVE_ARCH_HUGE_VMAP`
  - `SWIOTLB`
  - `ARCH_HAS_PTE_SPECIAL`
  - no `X86_5LEVEL`
  - no `HAVE_MEMBLOCK_PHYS_MAP`
  - no `DEFERRED_STRUCT_PAGE_INIT`
  - no `ARCH_HAS_SYNC_DMA_FOR_DEVICE`
  - no 32-bit specific tricks
    - no `X86_32`
    - no `HIGHMEM4G`
    - no `VMSPLIT_3G`
    - no `PAGE_OFFSET`
    - no `PAE`
    - no `HIGHPTE`
- `start_kernel`
  - `setup_arch`
    - set up memblock according to e820, `e820__memblock_setup`
      - `memblock_add` adds a memory region to memblock
      - `memblock_reserve` adds a reserved region to memblock
        - a range that is in the memory regions but not in the reserved
          regions are considered available
      - an allocation finds a range in the memory regions but not in the
      	reserved regions, and then adds the range to the reserved regions
    - `init_mem_mapping` initializes `PAGE_OFFSET` linear mapping
      - direct mappings to all physical memory regions; gaps between the
      	memory regions are not mapped
      - va `PAGE_OFFSET + x`  is mapped to pa `x` as long as x belong to a
      	physical memory region
      - space for page tables are taken from the BSS of the kernel image
    - `initmem_init` assigns all memory regions in memblock to node 0
    - `native_pagetable_init` is defined to `paging_init`
      - `sparse_memory_present_with_active_regions` calls `memory_available`
      	on all memory regions in memblock
	- with the default EXTREME mode, each root has `SECTIONS_PER_ROOT`
	  (256) sections and each section has `PAGES_PER_SECTION` (2^15) pages
	- it allocates 8MiB worth of memory from memblock for the roots, to
	  cover the entire 46-bit of physical memory address space
	- the number of "present" sections depend on the size of the physical
	  memory
      - `sparse_init` calls `sparse_mem_map_populate` (vmemmap version) for
      	each present section, which allocates page tables and `struct page`
      	from memblock, and set up mappings to access `struct page` from va
      	`vmemmap` region
	- va `vmemmap + sizeof(struct page) * pfn` is mapped to `struct page`
	  for pfn, if `pfn << PAGE_SHIFT` is an address of the phsycal memory
      - `zone_sizes_init` tells the buddy page allocator the sizes of memory
      	zones
  - `mm_init`
    - `mem_init`
      - `memblock_free_all` releases all pages from memblock to the buddy page
      	allocator.  Those in the reserved regions are marked `PG_reserved`.
    - `kmem_cache_init` initializes the slab allocator on top of the buddy
      allocator
    - `vmalloc_init` prepares for vmallocs/ioremaps that will use
      `VMALLOC_START` and onward (32TB)

## Memory Initialization (old)

- x86-64 uses 4-level page table.
  - `PGDIR_SHIFT == 39`
  - `PUD_SHIFT == 30`
  - `PMD_SHIFT == 21`.
  - `PTRS_PER_PGD` is 512, `1 << (48 - 39)`.
  - `PTRS_PER_PUD` is 512, `1 << (39 - 30)`.
  - `PTRS_PER_PMD` is 512, `1 << (30 - 21)`.
  - `PTRS_PER_PTE` is 512, `1 << (21 - 12)`.
- There are three memory allocators
  - brk
  - memblock
  - e820 and `alloc_low_page`
  - buddy allocator
- In `start_kernel`, after the linux banner is printed, `setup_arch` is called.
  - It calls `e820__memory_setup` to decide physical memory maps.
  - It calls `e820__end_of_ram_pfn` to decide 
    - `max_pfn`, (pfn of the last page of the DRAM) + 1.
    - `max_arch_pfn`, `1 << MAX_PHYSMEM_BITS`
  - It calls `e820__end_of_low_ram_pfn` to decide
    - `max_low_pfn`, which is `min(max_pfn, 4GB>>12)`
  - It calls `reserve_brk` to resert brk region.
  - It calls `init_mem_mapping` to map the low memory.
    - It runs before bootmem for early direct access.
    - The first range is the first 4MB (`PMD_SIZE`), using 4K page.
    - The second range is the rest, using 4M page
    - It calls `find_early_table_space` to reserve in `e820_table_start` and
      `e820_table_end` a place to store the page tables.
      - This storage is available to others through `alloc_low_page`.  It is
        the `MAPPING_BEYOND_END` mapped in `head_32.S` 
      - This storage is reserved by `reserve_early`.
    - The mappings are set up by `kernel_physical_mapping_init`.
    - cr3 is loaded, with `swapper_pg_dir` still being the PDE table.
    - `max_low_pfn_mapped` and `max_pfn_mapped` are set to `max_low_pfn`
      if everything goes normally.
    - That is, the mapped memory goes from 8MB to whole low memory.
  - It calls `initmem_init` to set up bootmem, boot-time physical memory
    allocator.
    - `e820_register_active_regions` is called with node 0 and all physical
      memory.  All e820 ram regions are stored in `early_node_map` of buddy
      allocator.
    - `setup_bootmem_allocator` is called.  It sets `after_bootmem` to 1.
    - It `reserve_early` an area for `bootmap`.
    - Later, `early_res_to_bootmem` is called to migrate early reserved area to
      bootmem map.
  - It calls `paging_init` to allocate an array of all `struct page *` from
    bootmem.
    - All pages are initialized as reserved at this point by `memmap_init_zone`.
- Much later in `start_kernel`, `mem_init` is called
  - It calls `free_all_bootmem` to return unused pages in bootmem to the buddy
    allocator.
  - `zap_low_mappings` is called to, forget low memory mappings.  That is,
    virtual address `< PAGE_OFFSET` is no longer mapped.  This is so that one
    can only access kernel memory from kernel address space.
- An example of early reserved regions

    (8 early reservations) ==> bootmem [0000000000 - 00377fe000]
      #0 [0000000000 - 0000001000]   BIOS data page ==> [0000000000 - 0000001000]
      #1 [0000001000 - 0000002000]    EX TRAMPOLINE ==> [0000001000 - 0000002000]
      #2 [0000006000 - 0000007000]       TRAMPOLINE ==> [0000006000 - 0000007000]
      #3 [0000100000 - 00004cb180]    TEXT DATA BSS ==> [0000100000 - 00004cb180]
      #4 [00004cc000 - 00004cf000]    INIT_PG_TABLE ==> [00004cc000 - 00004cf000]
      #5 [000009fc00 - 0000100000]    BIOS reserved ==> [000009fc00 - 0000100000]
      #6 [0000007000 - 0000008000]          PGTABLE ==> [0000007000 - 0000008000]
      #7 [0000008000 - 000000f000]          BOOTMAP ==> [0000008000 - 000000f000]
  - PGTABLE is one page because only the first 4M uses 4K page.  The rest uses
    4M page and does not need to allocate memory.

## brk section

- Introduced in `93dbda7cbcd70a0bd1a99f39f44a9ccde8ab9040`, 2009.
  - And modified several times after.
- In the linker script, `.brk` is reserved for `*(.brk_reservation)` and
  is delimited by `__brk_base` and `__brk_limit`.  It comes just after `.bss`
  and just before `.end`.
- `RESERVE_BRK` is used to reserve brk space
  - It creates `.brk_reservation` section with the given size
  - It is used to reserve space for init page tables and dmi
- When `head_32.S` creates `default_entry`, it creats page tables in `.brk`.
  - `_brk_end` marks the location after the page tables.
  - `extend_brk` is used to alloc space in brk.  It extends `_brk_end`.
    - It is used indirectly by `dmi_alloc` in `drivers/firmware/dmi_scan.c`.
- `reserve_brk` is called to reserve brk as `reserve_early`.  It also pins the
  brk as read-only.
  - It reserves up to `_brk_end`, not to `_brk_limit`.  Therefore, it is safe to
    `RESERVE_BRK` a large (safe) region.

## 32-bit kernel APM

- `arch/x86/kernel/apm_32.c`
- there is an apm thread polling the apm hw events.  If suspend or standy
  events are received, the kernel does the suspend/standby.
- there is `/proc/apm_bios` for userspace to check for apm status.
- emulation
  - emulate an apm bios on (arm) embedded system
  - depends on the PMU to provide `apm_get_power_status`
  - userspace can control suspend/standby too.
